from __future__ import absolute_import, division, print_function
import tensorflow as tf
import matplotlib.pyplot as plt
import  matplotlib
from keras import datasets


#ç”¨äºŽå¯è§†åŒ–çš„å‚æ•°
matplotlib.rcParams['font.size'] = 20
matplotlib.rcParams['figure.titlesize'] = 20
matplotlib.rcParams['figure.figsize'] = [9, 7]
matplotlib.rcParams['font.family'] = ['STKaiTi']
matplotlib.rcParams['axes.unicode_minus']=False

#é¢„å¤„ç†å‡½æ•°
"""
ä»Žkeras.datasets ä¸­ç».batch()åŽåŠ è½½çš„å›¾ç‰‡x shape ä¸º
[ð‘, 28,28]ï¼Œåƒç´ ä½¿ç”¨0~255 çš„æ•´å½¢è¡¨ç¤ºï¼›æ ‡æ³¨shape ä¸º[ð‘]ï¼Œå³é‡‡æ ·çš„æ•°å­—ç¼–ç æ–¹å¼ã€‚å®žé™…
çš„ç¥žç»ç½‘ç»œè¾“å…¥ï¼Œä¸€èˆ¬éœ€è¦å°†å›¾ç‰‡æ•°æ®æ ‡å‡†åŒ–åˆ°[0,1]æˆ–[âˆ’1,1]ç­‰0 é™„è¿‘åŒºé—´ï¼ŒåŒæ—¶æ ¹æ®ç½‘
ç»œçš„è®¾ç½®ï¼Œéœ€è¦å°†shape [28,28] çš„è¾“å…¥Reshape ä¸ºåˆæ³•çš„æ ¼å¼ï¼›å¯¹äºŽæ ‡æ³¨ä¿¡æ¯ï¼Œå¯ä»¥é€‰æ‹©
åœ¨é¢„å¤„ç†æ—¶è¿›è¡Œone-hot ç¼–ç ï¼Œä¹Ÿå¯ä»¥åœ¨è®¡ç®—è¯¯å·®æ—¶è¿›è¡Œone-hot ç¼–ç ã€‚
æ ¹æ®ä¸‹ä¸€èŠ‚çš„å®žæˆ˜è®¾å®šï¼Œæˆ‘ä»¬å°†MNIST å›¾ç‰‡æ•°æ®æ˜ å°„åˆ°ð‘¥ âˆˆ [0,1]åŒºé—´ï¼Œè§†å›¾è°ƒæ•´ä¸º
[ð‘, 28 âˆ— 28]ï¼›å¯¹äºŽæ ‡æ³¨yï¼Œæˆ‘ä»¬é€‰æ‹©åœ¨é¢„å¤„ç†å‡½æ•°é‡Œé¢è¿›è¡Œone-hot ç¼–ç 
"""
def preprocess(x, y):
    # [b, 28, 28], [b]
    print(x.shape, y.shape)
    x = tf.cast(x, dtype=tf.float32) / 255.
    x = tf.reshape(x, [-1, 28 * 28])
    y = tf.cast(y, dtype=tf.int32)
    y = tf.one_hot(y, depth=10)

    return x, y

#å¯¼å…¥æ‰‹å†™æ•°å­—å›¾ç‰‡æ•°æ®é›†ï¼Œç”¨äºŽåˆ†ç±»ä»»åŠ¡
"""
é€šè¿‡load_data()ä¼šè¿”å›žç›¸åº”æ ¼å¼çš„æ•°æ®ï¼Œå¯¹äºŽå›¾ç‰‡æ•°æ®é›†MNIST, CIFAR10 ç­‰ï¼Œä¼šè¿”å›ž2
ä¸ªtupleï¼Œç¬¬ä¸€ä¸ªtuple ä¿å­˜äº†ç”¨äºŽè®­ç»ƒçš„æ•°æ®x,y è®­ç»ƒé›†å¯¹è±¡ï¼›ç¬¬2 ä¸ªtuple åˆ™ä¿å­˜äº†ç”¨äºŽ
æµ‹è¯•çš„æ•°æ®x_test,y_test æµ‹è¯•é›†å¯¹è±¡ï¼Œæ‰€æœ‰çš„æ•°æ®éƒ½ç”¨Numpy.array å®¹å™¨æ‰¿è½½
"""
(x, y), (x_test, y_test) = datasets.mnist.load_data()
print('x:', x.shape, 'y:', y.shape, 'x test:', x_test.shape, 'y test:', y_test)

#è½¬æ¢æˆdatasetå¯¹è±¡
train_db = tf.data.Dataset.from_tensor_slices((x, y))

#éšæœºæ‰“æ•£
"""
é€šè¿‡ Dataset.shuffle(buffer_size)å·¥å…·å¯ä»¥è®¾ç½®Dataset å¯¹è±¡éšæœºæ‰“æ•£æ•°æ®ä¹‹é—´çš„é¡ºåºï¼Œ
é˜²æ­¢æ¯æ¬¡è®­ç»ƒæ—¶æ•°æ®æŒ‰å›ºå®šé¡ºåºäº§ç”Ÿï¼Œä»Žè€Œä½¿å¾—æ¨¡åž‹å°è¯•â€œè®°å¿†â€ä½æ ‡ç­¾ä¿¡æ¯
buffer_size æŒ‡å®šç¼“å†²æ± çš„å¤§å°ï¼Œä¸€èˆ¬è®¾ç½®ä¸ºä¸€ä¸ªè¾ƒå¤§çš„å‚æ•°å³å¯
"""
train_db = train_db.shuffle(1000)

#æ‰¹è®­ç»ƒã€‚ä¸€èˆ¬åœ¨ç½‘ç»œçš„è®¡ç®—è¿‡ç¨‹ä¸­ä¼šåŒæ—¶è®¡ç®—å¤šä¸ªæ ·æœ¬ï¼Œæˆ‘ä»¬æŠŠè¿™ç§è®­ç»ƒæ–¹å¼å«åšæ‰¹è®­ç»ƒï¼Œå…¶ä¸­æ ·æœ¬çš„æ•°é‡å«åšbatchsz
batchsz = 512
train_db = train_db.batch(batchsz)

#ç›´æŽ¥mapé¢„å¤„ç†å‡½æ•°å®Œæˆé¢„å¤„ç†
train_db = train_db.map(preprocess)

#å¾ªçŽ¯è®­ç»ƒ
"""
ä¸€èˆ¬æŠŠå®Œæˆä¸€ä¸ªbatch çš„æ•°æ®è®­ç»ƒï¼Œå«åšä¸€ä¸ªstepï¼›
æµ‹è¯•ç‰ˆ(20191108)
5.8 MNIST æµ‹è¯•å®žæˆ˜[åœ¨æ­¤å¤„é”®å…¥] 25
é€šè¿‡å¤šä¸ªstep æ¥å®Œæˆæ•´ä¸ªè®­ç»ƒé›†çš„ä¸€æ¬¡è¿­ä»£ï¼Œå«åšä¸€ä¸ªepochã€‚åœ¨å®žé™…è®­ç»ƒæ—¶ï¼Œé€šå¸¸éœ€è¦
å¯¹æ•°æ®é›†è¿­ä»£å¤šä¸ªepoch æ‰èƒ½å–å¾—è¾ƒå¥½åœ°è®­ç»ƒæ•ˆæžœï¼š

for epoch in range(20): # è®­ç»ƒEpoch æ•°
   for step, (x,y) in enumerate(train_db): # è¿­ä»£Step æ•°
       a = a + 1 # training...
"""
#ä¹Ÿå¯ä»¥é€šè¿‡è®¾ç½®
train_db = train_db.repeat(20)

#å¤„ç†æµ‹è¯•é›†
test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))
test_db = test_db.shuffle(1000).batch(batchsz).map(preprocess)

#nextè¿”å›žè¿­ä»£å™¨çš„ä¸‹ä¸€ä¸ªé¡¹ç›®
x, y = next(iter(train_db))
print('train sample:', x.shape, y.shape)

def main():
    # learning rate
    lr = 1e-2
    accs, losses = [], []

    # 784 => 512
    w1, b1 = tf.Variable(tf.random.normal([784, 256], stddev=0.1)), tf.Variable(tf.zeros([256]))
    # 512 => 256
    w2, b2 = tf.Variable(tf.random.normal([256, 128], stddev=0.1)), tf.Variable(tf.zeros([128]))
    # 256 => 10
    w3, b3 = tf.Variable(tf.random.normal([128, 10], stddev=0.1)), tf.Variable(tf.zeros([10]))

    for step, (x, y) in enumerate(train_db):

        # [b, 28, 28] => [b, 784]
        x = tf.reshape(x, (-1, 784))

        with tf.GradientTape() as tape:

            # layer1.
            h1 = x @ w1 + b1
            h1 = tf.nn.relu(h1)
            # layer2
            h2 = h1 @ w2 + b2
            h2 = tf.nn.relu(h2)
            # output
            out = h2 @ w3 + b3
            # out = tf.nn.relu(out)

            # compute loss
            # [b, 10] - [b, 10]
            loss = tf.square(y - out)
            # [b, 10] => scalar
            loss = tf.reduce_mean(loss)

        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])
        for p, g in zip([w1, b1, w2, b2, w3, b3], grads):
            p.assign_sub(lr * g)

        # print
        if step % 80 == 0:
            print(step, 'loss:', float(loss))
            losses.append(float(loss))

        if step % 80 == 0:
            # evaluate/test
            total, total_correct = 0., 0

            for x, y in test_db:
                # layer1.
                h1 = x @ w1 + b1
                h1 = tf.nn.relu(h1)
                # layer2
                h2 = h1 @ w2 + b2
                h2 = tf.nn.relu(h2)
                # output
                out = h2 @ w3 + b3
                # [b, 10] => [b]
                pred = tf.argmax(out, axis=1)
                # convert one_hot y to number y
                y = tf.argmax(y, axis=1)
                # bool type
                correct = tf.equal(pred, y)
                # bool tensor => int tensor => numpy
                total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()
                total += x.shape[0]

            print(step, 'Evaluate Acc:', total_correct / total)

            accs.append(total_correct / total)

    plt.figure()
    x = [i * 80 for i in range(len(losses))]
    plt.plot(x, losses, color='C0', marker='s', label='è®­ç»ƒ')
    plt.ylabel('MSE')
    plt.xlabel('Step')
    plt.legend()
    plt.savefig('train.svg')

    plt.figure()
    plt.plot(x, accs, color='C1', marker='s', label='æµ‹è¯•')
    plt.ylabel('å‡†ç¡®çŽ‡')
    plt.xlabel('Step')
    plt.legend()
    plt.savefig('test.svg')


if __name__ == '__main__':
    main()
















